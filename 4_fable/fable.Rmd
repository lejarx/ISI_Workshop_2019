---
title: "High dimensional time series analysis"
date: "robjhyndman.com/hdtsa"
author: "4. Automatic forecasting algorithms"
toc: true
output:
  binb::monash:
    colortheme: monashwhite
    fig_width: 7
    fig_height: 3.5
    includes:
      in_header: ../header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE,
  dev.args = list(pointsize = 11)
)
options(digits = 3, width = 69)
library(tidyverse)
library(fpp3)
usmelec <- as_tsibble(fpp2::usmelec) %>%
  rename(Month = index, Generation = value)
us_change <- readr::read_csv("https://otexts.com/fpp3/extrafiles/us_change.csv")  %>%
  mutate(Time = yearquarter(Time)) %>%
  as_tsibble(index = Time)
eu_retail <- as_tsibble(fpp2::euretail)
h02 <- tsibbledata::PBS %>%
  filter(ATC2 == "H02") %>%
  summarise(Cost = sum(Cost))
```

# Exponential smoothing

## Historical perspective

 * Developed in the 1950s and 1960s as methods (algorithms) to produce point forecasts.
 * Combine a "level", "trend" (slope) and "seasonal" component to describe a time series.
 * The rate of change of the components are controlled by "smoothing parameters":\newline $\alpha$, $\beta$ and $\gamma$ respectively.
  * Need to choose best values for the smoothing parameters (and initial states).
  * Equivalent ETS state space models developed in the 1990s and 2000s.

## A model for levels, trends, and seasonalities
\fontsize{13}{14}\sf

We want a model that captures the level ($\ell_t$), trend ($b_t$) and seasonality ($s_t$).

\alert{How do we combine these elements?}

\pause

\begin{block}{Additively?}
$y_t = \ell_{t-1} + b_{t-1} + s_{t-m} + \varepsilon_t$
\end{block}\pause
\begin{block}{Multiplicatively?}
$y_t = \ell_{t-1}b_{t-1}s_{t-m}(1 + \varepsilon_t)$
\end{block}\pause
\begin{block}{Perhaps a mix of both?}
$y_t = (\ell_{t-1} + b_{t-1}) s_{t-m} + \varepsilon_t$
\end{block}\pause

\alert{How do the level, trend and seasonal components evolve over time?}

## ETS models

\begin{block}{}
\hspace*{-0.25cm}\begin{tabular}{l@{}p{2.3cm}@{}c@{}l}
\structure{General n\rlap{otation}}
    &       & ~E T S~  & ~:\hspace*{0.3cm}\textbf{E}xponen\textbf{T}ial \textbf{S}moothing               \\ [-0.2cm]
    & \hfill{$\nearrow$\hspace*{-0.1cm}}        & {$\uparrow$} & {\hspace*{-0.2cm}$\nwarrow$} \\
    & \hfill{\textbf{E}rror\hspace*{0.2cm}} & {\textbf{T}rend}      & {\hspace*{0.2cm}\textbf{S}eason}
\end{tabular}
\end{block}

\alert{\textbf{E}rror:} Additive (`"A"`) or multiplicative (`"M"`)
\pause

\alert{\textbf{T}rend:} None (`"N"`), additive (`"A"`), multiplicative (`"M"`), or damped (`"Ad"` or `"Md"`).
\pause

\alert{\textbf{S}easonality:} None (`"N"`), additive (`"A"`) or multiplicative (`"M"`)

## ETS(A,N,N): SES with additive errors

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T \\
\text{Measurement equation}&& y_t &= \ell_{t-1} + \varepsilon_t\\
\text{State equation}&& \ell_t&=\ell_{t-1}+\alpha \varepsilon_t
\end{align*}
\end{block}
where $\varepsilon_t\sim\text{NID}(0,\sigma^2)$.
\pause

  * "innovations" or "single source of error" because equations have the same error process, $\varepsilon_t$.
  * Measurement equation: relationship between observations and states.
  * Transition/state equation(s): evolution of the state(s) through time.

\vspace*{10cm}

## ETS(M,N,N): SES with multiplicative errors

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T \\
\text{Measurement equation}&& y_t &= \ell_{t-1}(1 + \varepsilon_t)\\
\text{State equation}&& \ell_t&=\ell_{t-1}(1+\alpha \varepsilon_t)
\end{align*}
\end{block}
where $\varepsilon_t\sim\text{NID}(0,\sigma^2)$.
\pause

  * Models with additive and multiplicative errors with the same parameters generate the same point forecasts but different prediction intervals.

\vspace*{10cm}

## ETS(A,A,N): Holt's linear trend

\begin{block}{Additive errors}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T + hb_T\\
\text{Measurement equation}&& y_t &= \ell_{t-1}+b_{t-1}+\varepsilon_t\\
\text{State equations}&&       \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
&&      b_t&=b_{t-1}+\beta \varepsilon_t
\end{align*}
\end{block}
\pause

\begin{block}{Multiplicative errors}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T + hb_T\\
\text{Measurement equation}&& y_t &= (\ell_{t-1}+b_{t-1})(1+\varepsilon_t)\\
\text{State equations}&&       \ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
&&      b_t&=b_{t-1}+\beta \varepsilon_t
\end{align*}
\end{block}

## Example: Australian population

\fontsize{9}{9}\sf

```{r holt-fit, echo=TRUE}
aus_economy <- global_economy %>% filter(Code == "AUS") %>%
  mutate(Pop = Population/1e6)
fit <- aus_economy %>% model(AAN = ETS(Pop))
report(fit)
```

## Example: Australian population

\fontsize{10}{11}\sf

```{r holt-cmp, echo=TRUE, dependson='holt-fit'}
components(fit)
```

## Example: Australian population

\fontsize{10}{11}\sf

```{r holt-cmp-plot, echo=TRUE, dependson='holt-fit', fig.height=4.5}
components(fit) %>% autoplot()
```

## Example: Australian population

\fontsize{12}{12}\sf

```{r holt-fc, echo=TRUE, cache=TRUE, dependson='holt-fit'}
fit %>%
  forecast(h = 20) %>%
  autoplot(aus_economy) +
  ylab("Population") + xlab("Year")
```

## ETS(A,Ad,N): Damped trend method
\fontsize{14}{16}\sf

\begin{block}{Additive errors}\vspace*{-0.2cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T + (\phi + \cdots + \phi^{h-1})b_T\\
\text{Measurement equation}&& y_t &= (\ell_{t-1}+\phi b_{t-1})+\varepsilon_t\\
\text{State equations}&&       \ell_t&=(\ell_{t-1}+\phi b_{t-1})+\alpha \varepsilon_t\\
&&      b_t&=\phi b_{t-1}+\beta \varepsilon_t
\end{align*}
\end{block}
\pause

  * Damping parameter $0<\phi<1$.
  * If $\phi=1$, identical to Holt's linear trend.
  * As $h\rightarrow\infty$, $\pred{y}{T+h}{T}\rightarrow \ell_T+\phi b_T/(1-\phi)$.
  * Short-run forecasts trended, long-run forecasts constant.

## Example: Australian population
\fontsize{12}{12}\sf

```{r, echo=TRUE, fig.height=3.6}
aus_economy %>%
  model(holt = ETS(Pop ~ trend("Ad"))) %>%
  forecast(h = 20) %>%
  autoplot(aus_economy)
```

## Example: National populations

\fontsize{9}{9}\sf

```{r popfit, echo=TRUE, cache=TRUE}
fit <- global_economy %>%
  mutate(Pop = Population/1e6) %>%
  model(ets = ETS(Pop))
fit
```

## Example: National populations
\fontsize{12}{12}\sf

```{r popfc, echo=TRUE, cache=TRUE, dependson="popfit"}
fit %>%
  forecast(h = 5)
```

## \fontsize{16}{16}\sf\bfseries ETS(A,A,A): Holt-Winters additive method

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation} && \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)}\\
\text{Observation equation}&& y_t&=\ell_{t-1}+b_{t-1}+s_{t-m} + \varepsilon_t\\
\text{State equations}&& \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
&&        b_t&=b_{t-1}+\beta \varepsilon_t \\
&&s_t &= s_{t-m} + \gamma\varepsilon_t
\end{align*}
\end{block}

* $k=$ integer part of $(h-1)/m$.
* $\sum_i s_i \approx 0$.
* Parameters:&nbsp; $0\le \alpha\le 1$,&nbsp; $0\le \beta^*\le 1$,&nbsp; $0\le \gamma\le 1-\alpha$&nbsp;  and $m=$  period of seasonality (e.g. $m=4$ for quarterly data).

## \fontsize{16}{16}\sf\bfseries ETS(M,A,M): Holt-Winters multiplicative method

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation} && \hat{y}_{t+h|t} &= (\ell_{t} + hb_{t}) s_{t+h-m(k+1)}\\
\text{Observation equation}&& y_t&= (\ell_{t-1}+b_{t-1})s_{t-m}(1 + \varepsilon_t)\\
\text{State equations}&& \ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
&&        b_t&=b_{t-1}(1+\beta \varepsilon_t) \\
&&s_t &= s_{t-m}(1 + \gamma\varepsilon_t)
\end{align*}
\end{block}

* $k$ is integer part of $(h-1)/m$.
* $\sum_i s_i \approx m$.
* Parameters:&nbsp; $0\le \alpha\le 1$,&nbsp; $0\le \beta^*\le 1$,&nbsp; $0\le \gamma\le 1-\alpha$&nbsp;  and $m=$  period of seasonality (e.g. $m=4$ for quarterly data).

## Example: Australian holiday tourism

\fontsize{9}{10}\sf

```{r ausholidays-fit, echo=TRUE}
holidays <- tourism %>%
  filter(Purpose == "Holiday")
fit <- holidays %>% model(ets = ETS(Trips))
fit
```


## Example: Australian holiday tourism

\fontsize{9}{10}\sf

```{r ausholidays-report}
fit %>% filter(Region=="Snowy Mountains") %>% report()
```

## Example: Australian holiday tourism

\fontsize{9}{10}\sf

```{r ausholidays-components}
fit %>% filter(Region=="Snowy Mountains") %>% components(fit)
```

## Example: Australian holiday tourism

\fontsize{9}{10}\sf

```{r ausholidays-components-plot, fig.height=4.3}
fit %>% filter(Region=="Snowy Mountains") %>%
  components(fit) %>% autoplot()
```

## Example: Australian holiday tourism

\fontsize{9}{10}\sf

```{r ausholidays-forecast}
fit %>% forecast()
```

## Example: Australian holiday tourism

\fontsize{9}{10}\sf

```{r ausholidays-forecast-plot}
fit %>% forecast() %>%
  filter(Region=="Snowy Mountains") %>%
  autoplot(holidays) +
    xlab("Year") + ylab("Overnight trips (thousands)")
```


## Exponential smoothing models
\fontsize{11}{12}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Additive Error}} &        \multicolumn{3}{c}{\bf Seasonal Component}         \\
          \multicolumn{2}{c|}{\bf Trend}         &         N         &         A         &         M         \\
        \multicolumn{2}{c|}{\bf Component}       &     ~(None)~      &    (Additive)     & (Multiplicative)  \\ \cline{3-5}
           &                                     &                   &                   &  \\[-0.3cm]
  N        & (None)                              &       A,N,N       &       A,N,A       &    \st{A,N,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A        & (Additive)                          &       A,A,N       &       A,A,A       &    \st{A,A,M}     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                   &   A,A\damped,N    &   A,A\damped,A    & \st{A,A\damped,M}
\end{tabular}
\end{block}

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Multiplicative Error}} &     \multicolumn{3}{c}{\bf Seasonal Component}      \\
             \multicolumn{2}{c|}{\bf Trend}            &      N       &         A         &        M         \\
           \multicolumn{2}{c|}{\bf Component}          &   ~(None)~   &    (Additive)     & (Multiplicative) \\ \cline{3-5}
           &                                           &              &                   &  \\[-0.3cm]
  N        & (None)                                    &    M,N,N     &       M,N,A       &      M,N,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A        & (Additive)                                &    M,A,N     &       M,A,A       &      M,A,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                         & M,A\damped,N &   M,A\damped,A    &   M,A\damped,M
\end{tabular}
\end{block}

## Estimating ETS models

  * Smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$, and the initial states $\ell_0$, $b_0$, $s_0,s_{-1},\dots,s_{-m+1}$ are estimated by maximising the "likelihood" = the probability of the data arising from the specified model.
  * For models with additive errors equivalent to minimising SSE.
  * For models with multiplicative errors, \textbf{not} equivalent to minimising SSE.

## Model selection
\fontsize{13}{15}\sf

\begin{block}{Akaike's Information Criterion}
\[
\text{AIC} = -2\log(\text{L}) + 2k
\]
\end{block}\vspace*{-0.2cm}
where $L$ is the likelihood and $k$ is the number of parameters initial states estimated in the model.\pause

\begin{block}{Corrected AIC}
\[
\text{AIC}_{\text{c}} = \text{AIC} + \frac{2(k+1)(k+2)}{T-k}
\]
\end{block}
which is the AIC corrected (for small sample bias).
\pause
\begin{block}{Bayesian Information Criterion}
\[
\text{BIC} = \text{AIC} + k(\log(T)-2).
\]
\end{block}

## AIC and cross-validation

\Large

\begin{alertblock}{}
Minimizing the AIC assuming Gaussian residuals is asymptotically equivalent to minimizing one-step time series cross validation MSE.
\end{alertblock}

## Automatic forecasting

**From Hyndman et al.\ (IJF, 2002):**

1. Apply each model that is appropriate to the data.
Optimize parameters and initial values using MLE.
1. Select best method using AICc.
1. Produce forecasts using best method.
1. Obtain forecast intervals using underlying state space model.

* Method performed very well in M3 competition.
* Used as a benchmark in the M4 competition.

# Lab Session 7

## Lab Session 7

1. Find an ETS model for the Gas data from `aus_production`.

    * Why is multiplicative seasonality necessary here?
    * Experiment with making the trend damped.

2. Use `ETS()` on some of these series: `tourism`, `gafa_stock`, `pelt`.

    * Does it always give good forecasts?
    * Find an example where it does not work well. Can you figure out why?

# ARIMA models

## ARIMA models

\begin{tabular}{rl}
\textbf{AR}: & autoregressive (lagged observations as inputs)\\
\textbf{I}: & integrated (differencing to make series stationary)\\
\textbf{MA}: & moving average (lagged errors as inputs)
\end{tabular}

\pause

###
An ARIMA model is rarely interpretable in terms of visible data structures like trend and seasonality. But it can capture a huge range of time series patterns.

## Stationarity

\begin{block}{Definition}
If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause

A **stationary series** is:

* roughly horizontal
* constant variance
* no patterns predictable in the long-term

## Stationary?
\fontsize{11}{12}\sf

```{r}
gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) == 2018) %>%
  autoplot(Close) +
    ylab("Google closing stock price ($US)")
```

## Stationary?
\fontsize{11}{12}\sf

```{r}
gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) == 2018) %>%
  autoplot(difference(Close)) +
    ylab("Daily change in Google closing stock price")
```

## Differencing
\fontsize{13}{15}\sf

* Differencing helps to **stabilize the mean**.
* The differenced series is the *change* between each observation in the original series.
* Occasionally the differenced data will not appear stationary and it may be necessary to difference the data a second time.
* In practice, it is almost never necessary to go beyond second-order differences.

## Autoregressive models

\begin{block}{Autoregressive (AR) models:}\vspace*{-0.4cm}
$$
  y_{t} = c + \phi_{1}y_{t - 1} + \phi_{2}y_{t - 2} + \cdots + \phi_{p}y_{t - p} + \varepsilon_{t},
$$
where $\varepsilon_t$ is white noise. This is a multiple regression with \textbf{lagged values} of $y_t$ as predictors.
\end{block}

```{r arp, echo=FALSE, fig.height=2.5}
set.seed(1)
p1 <- tsibble(idx = seq_len(100), sim = 10 + arima.sim(list(ar = -0.8), n = 100), index = idx) %>%
  autoplot(sim) + xlab("time") + ylab("") + ggtitle("AR(1)")
p2 <- tsibble(idx = seq_len(100), sim = 20 + arima.sim(list(ar = c(1.3, -0.7)), n = 100), index = idx) %>%
  autoplot(sim) + xlab("time") + ylab("") + ggtitle("AR(2)")
gridExtra::grid.arrange(p1,p2,nrow=1)
```

* Cyclic behaviour is possible when $p\ge 2$.

## Moving Average (MA) models

\begin{block}{Moving Average (MA) models:}\vspace*{-0.3cm}
$$
  y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t - 1} + \theta_{2}\varepsilon_{t - 2} + \cdots + \theta_{q}\varepsilon_{t - q},
$$
where $\varepsilon_t$ is white noise.
This is a multiple regression with \textbf{lagged \emph{errors}} as predictors. \emph{Don't confuse this with moving average smoothing!}
\end{block}

```{r maq, fig.height=2.5, echo=FALSE}
set.seed(2)
p1 <- tsibble(idx = seq_len(100), sim = 20 + arima.sim(list(ma = 0.8), n = 100), index = idx) %>%
  autoplot(sim) + xlab("time") + ylab("") + ggtitle("MA(1)")
p2 <- tsibble(idx = seq_len(100), sim = arima.sim(list(ma = c(-1, +0.8)), n = 100), index = idx) %>%
  autoplot(sim) + xlab("time") + ylab("") + ggtitle("MA(2)")

gridExtra::grid.arrange(p1,p2,nrow=1)
```


## ARIMA models

\begin{block}{Autoregressive Moving Average models:}\vspace*{-0.4cm}
\begin{align*}
  y_{t} &= c + \phi_{1}y_{t - 1} + \cdots + \phi_{p}y_{t - p} \\
        & \hspace*{2.4cm}\text{} + \theta_{1}\varepsilon_{t - 1} + \cdots + \theta_{q}\varepsilon_{t - q} + \varepsilon_{t}.
\end{align*}
\end{block}\pause

* Predictors include both **lagged values of $y_t$ and lagged errors.**
\pause

### Autoregressive Integrated Moving Average models
* Combine ARMA model with **differencing**.
* $d$-differenced series follows an ARMA model.
* Need to choose $p$, $d$, $q$ and whether or not to include $c$.

## ARIMA models

\begin{block}{ARIMA($p, d, q$) model}
\begin{tabular}{rl}
AR:& $p =$ order of the autoregressive part\\
I: & $d =$ degree of first differencing involved\\
MA:& $q =$ order of the moving average part.
\end{tabular}
\end{block}

* White noise model: ARIMA(0,0,0)
* Random walk: ARIMA(0,1,0) with no constant
* Random walk with drift: ARIMA(0,1,0) with \rlap{const.}
* AR($p$): ARIMA($p$,0,0)
* MA($q$): ARIMA(0,0,$q$)

## Annual lynx trappings
\fontsize{11}{12}\sf

```{r pelt}
pelt %>% autoplot(Lynx) +
  xlab("Year") + ylab("Number trapped") +
  ggtitle("Annual Canadian Lynx Trappings")
```

## Annual lynx trappings
\fontsize{11}{12}\sf

```{r peltsqrt}
pelt %>% autoplot(sqrt(Lynx)) +
  xlab("Year") + ylab("Number trapped") +
  ggtitle("Annual Canadian Lynx Trappings")
```

## Annual lynx trappings
\fontsize{11}{12}\sf

```{r pelt1}
pelt %>%
  model(lynx = ARIMA(sqrt(Lynx))) %>%
  report()
```

## Annual lynx trappings
\fontsize{11}{12}\sf

```{r pelt2}
pelt %>%
  model(lynx = ARIMA(sqrt(Lynx))) %>%
  forecast(h=20) %>%
  autoplot(pelt)
```

## Example: National populations
\fontsize{9}{9}\sf

```{r popfit2, echo=TRUE, cache=TRUE}
fit <- global_economy %>%
  mutate(Pop = Population/1e6) %>%
  model(arima = ARIMA(Pop))
fit
```

## Understanding ARIMA models
\fontsize{14}{16}\sf

* If $c=0$ and $d=0$, the long-term forecasts will go to zero.
* If $c=0$ and $d=1$, the long-term forecasts will go to a non-zero constant.
* If $c=0$ and $d=2$, the long-term forecasts will follow a straight line.

* If $c\ne0$ and $d=0$, the long-term forecasts will go to the mean of the data.
* If $c\ne0$ and $d=1$, the long-term forecasts will follow a straight line.
* If $c\ne0$ and $d=2$, the long-term forecasts will follow a quadratic trend.

## Understanding ARIMA models
\fontsize{14}{15.5}\sf

### Forecast variance and $d$
  * The higher the value of $d$, the more rapidly the prediction intervals increase in size.
  * For $d=0$, the long-term forecast standard deviation will go to the standard deviation of the historical data.


## How does ARIMA() work?

\begin{alertblock}{Hyndman and Khandakar (JSS, 2008) algorithm:}
\begin{itemize}\tightlist
\item Select no.\ differences $d$ via KPSS test.
\item Select $p$, $q$ and inclusion of $c$ by minimising AICc.
\item Use stepwise search to traverse model space.
\end{itemize}
\end{alertblock}\pause

\begin{block}{}
$$\text{AICc} = -2 \log(L) + 2(p+q+k+1)\left[1 + \frac{(p+q+k+2)}{T-p-q-k-2}\right].$$
where $L$ is the maximised likelihood fitted to the \textit{differenced} data,
$k=1$ if $c\neq 0$ and $k=0$ otherwise.\pause
\end{block}

Note: Can't compare AICc for different values of $d$.

## How does ARIMA() work?
\fontsize{12.5}{14.5}\sf

Step1:
: Select current model (with smallest AICc) from:\newline
ARIMA$(2,d,2)$\newline
ARIMA$(0,d,0)$\newline
ARIMA$(1,d,0)$\newline
ARIMA$(0,d,1)$
\pause\vspace*{-0.1cm}

Step 2:
: Consider variations of current model:

    * vary one of $p,q,$ from current model by $\pm1$;
    * $p,q$ both vary from current model by $\pm1$;
    * Include/exclude $c$ from current model.

  Model with lowest AICc becomes current model.

\pause\alert{Repeat Step 2 until no lower AICc can be found.}

# Lab Session 9

## Lab Session 9

For the United States GDP data (from `global_economy`):

 * Fit a suitable ARIMA model for the logged data.
 * Produce forecasts of your fitted model. Do the forecasts look reasonable?

# Seasonal ARIMA models

## Electricity production
\fontsize{11}{13}\sf

```{r, echo=TRUE, fig.height=3.5}
usmelec %>% autoplot(
  Generation
)
```

## Electricity production
\fontsize{11}{13}\sf

```{r, echo=TRUE, fig.height=3.5}
usmelec %>% autoplot(
  log(Generation)
)
```

## Electricity production
\fontsize{11}{13}\sf

```{r, echo=TRUE, fig.height=3.5}
usmelec %>% autoplot(
  log(Generation) %>% difference(12)
)
```

## Electricity production
\fontsize{11}{13}\sf

```{r, echo=TRUE, fig.height=3.5}
usmelec %>% autoplot(
  log(Generation) %>% difference(12) %>% difference()
)
```

## Example: US electricity production
\fontsize{11}{13}\sf

```{r usmelec0, echo=TRUE, fig.height=3.5}
usmelec %>% autoplot(Generation)
```

## Example: US electricity production
\fontsize{11}{13}\sf

```{r usmelec1, echo=TRUE, fig.height=3.5}
usmelec %>% autoplot(log(Generation))
```


## Example: US electricity production
\fontsize{11}{13}\sf

```{r usmelec2, echo=TRUE}
usmelec %>%
  model(arima = ARIMA(log(Generation))) %>%
  report()
```

## Example: US electricity production
\fontsize{11}{13}\sf

```{r usmelec3, echo=TRUE, fig.height=3.2}
usmelec %>%
  model(arima = ARIMA(log(Generation))) %>%
  forecast(h="3 years") %>%
  autoplot(usmelec)
```

## Example: US electricity production
\fontsize{11}{13}\sf

```{r usmelec4, echo=TRUE, fig.height=3.2}
usmelec %>%
  model(arima = ARIMA(log(Generation))) %>%
  forecast(h="3 years") %>%
  autoplot(filter_index(usmelec, 2005 ~ .))
```

## Seasonal ARIMA models

| ARIMA | $~\underbrace{(p, d, q)}$ | $\underbrace{(P, D, Q)_{m}}$ |
| ----: | :-----------------------: | :--------------------------: |
|       | ${\uparrow}$              | ${\uparrow}$                 |
|       | Non-seasonal part         | Seasonal part of             |
|       | of the model              | of the model                 |

where $m =$ number of observations per year.

 * $d$ first differences
 * $D$ seasonal differences
 * $p$ AR lags
 * $q$ MA lags
 * $P$ seasonal AR lags
 * $Q$ seasonal MA lags

Note: seasonal and non-seasonal terms combine multiplicatively

## Common ARIMA models

The US Census Bureau uses the following models most often:\vspace*{0.5cm}

\begin{tabular}{|ll|}
\hline
ARIMA(0,1,1)(0,1,1)$_m$& with log transformation\\
ARIMA(0,1,2)(0,1,1)$_m$& with log transformation\\
ARIMA(2,1,0)(0,1,1)$_m$& with log transformation\\
ARIMA(0,2,2)(0,1,1)$_m$& with log transformation\\
ARIMA(2,1,2)(0,1,1)$_m$& with no transformation\\
\hline
\end{tabular}

## Cortecosteroid drug sales

```{r h02}
h02 %>%
  mutate(log(Cost)) %>%
  gather() %>%
  ggplot(aes(x = Month, y = value)) +
  geom_line() +
  facet_grid(key ~ ., scales = "free_y") +
  xlab("Year") + ylab("") +
  ggtitle("Cortecosteroid drug scripts (H02)")
```

## Cortecosteroid drug sales
```{r h02b}
h02 %>% gg_tsdisplay(difference(log(Cost),12), lag_max = 36)
```

## Cortecosteroid drug sales

  * Choose $D=1$ and $d=0$.
  * Spikes in PACF at lags 12 and 24 suggest seasonal AR(2) term.
  * Spikes in PACF suggests possible non-seasonal AR(3) term.
  * Initial candidate model: ARIMA(3,0,0)(2,1,0)$_{12}$.

## Cortecosteroid drug sales

```{r h02aicc, echo=FALSE}
models <- list(
  c(3,0,0,2,1,0),
  c(3,0,1,2,1,0),
  c(3,0,2,2,1,0),
  c(3,0,1,1,1,0),
  c(3,0,1,0,1,1),
  c(3,0,1,0,1,2),
  c(3,0,1,1,1,1)
)
model_defs <- map(models, ~ ARIMA(log(Cost) ~ 0 + pdq(!!.[1], !!.[2], !!.[3]) + PDQ(!!.[4], !!.[5], !!.[6])))
model_defs <- set_names(model_defs, map_chr(models,
  ~ sprintf("ARIMA(%i,%i,%i)(%i,%i,%i)[12]", .[1], .[2], .[3], .[4], .[5], .[6])))

fit <- h02 %>%
  model(!!!model_defs)

fit %>%
  glance %>%
  arrange(AICc) %>%
  select(.model, AICc) %>%
  knitr::kable(digits=2, row.names=FALSE, align='cc', booktabs=TRUE)
```

## Cortecosteroid drug sales
\fontsize{10}{12}\sf

```{r arimah02, echo=TRUE}
fit <- h02 %>%
  model(best = ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2)))
report(fit)
```

## Cortecosteroid drug sales
\fontsize{10}{12}\sf

```{r h02res, echo=TRUE, fig.height=4, dependson='arimah02'}
augment(fit) %>%
  gg_tsdisplay(.resid, lag_max=36, plot_type = "hist")
```

## Cortecosteroid drug sales
\fontsize{11}{15}\sf

```{r h02resb, echo = TRUE, fig.height=4, dependson='arimah02'}
augment(fit) %>%
  features(.resid, ljung_box, lag = 36, dof = 6)
```

## Cortecosteroid drug sales
\fontsize{11}{13}\sf

```{r h02auto, echo=TRUE, fig.height=3.6}
fit <- h02 %>% model(auto = ARIMA(log(Cost)))
report(fit)
```

## Cortecosteroid drug sales
\fontsize{10}{12}\sf

```{r, echo=TRUE, fig.height=4, dependson='h02auto'}
augment(fit) %>%
  gg_tsdisplay(.resid, lag_max = 36, plot_type = "hist")
```

## Cortecosteroid drug sales
\fontsize{11}{15}\sf

```{r, echo = TRUE, dependson='h02auto'}
augment(fit) %>%
  features(.resid, ljung_box, lag = 36, dof = 5)
```

## Cortecosteroid drug sales
\fontsize{9}{9}\sf

```{r h02tryharder, echo=TRUE, fig.height=3.6}
fit <- h02 %>%
  model(best = ARIMA(log(Cost), stepwise = FALSE,
                 approximation = FALSE,
                 order_constraint = p + q + P + Q <= 9))
report(fit)
```

## Cortecosteroid drug sales
\fontsize{10}{12}\sf

```{r, echo=TRUE, fig.height=4, dependson='h02tryharder'}
augment(fit) %>%
  gg_tsdisplay(.resid, lag_max = 36, plot_type = "hist")
```

## Cortecosteroid drug sales
\fontsize{11}{15}\sf

```{r, echo = TRUE, dependson='h02tryharder'}
augment(fit) %>%
  features(.resid, ljung_box, lag = 36, dof = 9)
```

## Cortecosteroid drug sales
\fontsize{10}{12}\sf

Training data: July 1991 to June 2006

Test data: July 2006--June 2008

```r
fit <- h02 %>%
  filter_index(~ "2006 Jun") %>%
  model(
    ARIMA(log(Cost) ~ pdq(3, 0, 0) + PDQ(2, 1, 0)),
    ARIMA(log(Cost) ~ pdq(3, 0, 1) + PDQ(2, 1, 0)),
    ARIMA(log(Cost) ~ pdq(3, 0, 2) + PDQ(2, 1, 0)),
    ARIMA(log(Cost) ~ pdq(3, 0, 1) + PDQ(1, 1, 0))
    # ... #
  )

fit %>%
  forecast(h = "2 years") %>%
  accuracy(h02 %>% filter_index("2006 Jul" ~ .))
```

## Cortecosteroid drug sales
\fontsize{12}{14}\sf

```{r h02-rmse, cache=TRUE}
models <- list(
  c(3,0,0,2,1,0),
  c(3,0,1,2,1,0),
  c(3,0,2,2,1,0),
  c(3,0,1,1,1,0),
  c(3,0,1,0,1,1),
  c(3,0,1,0,1,2),
  c(3,0,1,1,1,1),
  c(3,0,3,0,1,1),
  c(3,0,2,0,1,1),
  c(2,1,3,0,1,1),
  c(2,1,4,0,1,1),
  c(2,1,5,0,1,1),
  c(4,1,1,2,1,2))

model_defs <- map(models, ~ ARIMA(log(Cost) ~ 0 + pdq(!!.[1], !!.[2], !!.[3]) + PDQ(!!.[4], !!.[5], !!.[6])))
model_defs <- set_names(model_defs, map_chr(models,
  ~ sprintf("ARIMA(%i,%i,%i)(%i,%i,%i)[12]", .[1], .[2], .[3], .[4], .[5], .[6])))

fit <- h02 %>%
  filter_index(~ "2006 Jun") %>%
  model(!!!model_defs)

fit %>%
  forecast(h = "2 years") %>%
  accuracy(h02 %>% filter_index("2006 Jul" ~ .)) %>%
  arrange(RMSE) %>%
  select(.model, RMSE) %>%
  knitr::kable(digits=4)
```

## Cortecosteroid drug sales

  * Models with lowest AICc values tend to give slightly better results than the other models.
  * AICc comparisons must have the same orders of differencing. But RMSE test set comparisons can involve any models.
  * Use the best model available, even if it does not pass all tests.

## Cortecosteroid drug sales
\fontsize{11}{14}\sf

```{r h02f, echo=TRUE, fig.height=3}
fit <- h02 %>%
  model(ARIMA(Cost ~ 0 + pdq(3,0,1) + PDQ(0,1,2)))
fit %>% forecast %>% autoplot(h02) +
  ylab("H02 Expenditure ($AUD)") + xlab("Year")
```


# ARIMA vs ETS

## ARIMA vs ETS

  * Myth that ARIMA models are more general than exponential smoothing.

  * Linear exponential smoothing models all special cases of ARIMA models.

  * Non-linear exponential smoothing models have no equivalent ARIMA counterparts.

  * Many ARIMA models have no exponential smoothing counterparts.

  * ETS models all non-stationary. Models with seasonality or non-damped trend (or both) have two unit roots; all other models have one unit \rlap{root.}

\vspace*{10cm}

## Equivalences

\fontsize{12}{14}\sf

|**ETS model**  | **ARIMA model**             | **Parameters**                       |
| :------------ | :-------------------------- | :----------------------------------- |
| ETS(A,N,N)    | ARIMA(0,1,1)                | $\theta_1 = \alpha-1$                |
| ETS(A,A,N)    | ARIMA(0,2,2)                | $\theta_1 = \alpha+\beta-2$          |
|               |                             | $\theta_2 = 1-\alpha$                |
| ETS(A,A,N)    | ARIMA(1,1,2)                | $\phi_1=\phi$                        |
|               |                             | $\theta_1 = \alpha+\phi\beta-1-\phi$ |
|               |                             | $\theta_2 = (1-\alpha)\phi$          |
| ETS(A,N,A)    | ARIMA(0,0,$m$)(0,1,0)$_m$   |                                      |
| ETS(A,A,A)    | ARIMA(0,1,$m+1$)(0,1,0)$_m$ |                                      |
| ETS(A,A,A)    | ARIMA(1,0,$m+1$)(0,1,0)$_m$ |                                      |

# Lab Session 10
## Lab Session 10


\fontsize{13}{13}\sf

For the `fma::condmilk` series:

  * Do the data need transforming? If so, find a suitable transformation.
  * Are the data stationary? If not, find an appropriate differencing which yields stationary data.
  * Identify a couple of ARIMA models that might be useful in describing the time series.
  * Which of your models is the best according to their AIC values?
  * Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.
  * Forecast the next 24 months of data using your preferred model.
  * Compare the forecasts obtained using `ets()`.
